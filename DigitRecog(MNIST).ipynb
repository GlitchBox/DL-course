{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/mnist-numpy/mnist.npz\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#I will use Keras deep learning library to implement ConvNet \nfrom keras.datasets import mnist\nimport matplotlib.pyplot as plt\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data(\"/kaggle/input/mnist-numpy/mnist.npz\")\n\n#plot 4 images as gray scale\nplt.subplot(221)\nplt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n# plt.subplot(422)\n# plt.imshow(X_train[0])\n\nplt.subplot(222)\nplt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n\n# plt.subplot(424)\n# plt.imshow(X_train[1])\n\nplt.subplot(223)\nplt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n\n# plt.subplot(426)\n# plt.imshow(X_train[2])\n\nplt.subplot(224)\nplt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n\n# plt.subplot(428)\n# plt.imshow(X_train[3])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#following is a simple multi layer neural network for digit recognition\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data('/kaggle/input/mnist-numpy/mnist.npz')\nprint(\"X_train,Y_train:\")\nprint(str(X_train.shape)+\",\"+str(Y_train.shape))\nprint(\"X_test, Y_test\")\nprint(str(X_test.shape)+\", \"+str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pixelNum = X_train.shape[1]*X_train.shape[2]\nprint(pixelNum)\nX_train = X_train.reshape((X_train.shape[0], pixelNum)).astype('float32')\nX_test = X_test.reshape((X_test.shape[0], pixelNum)).astype('float32')\n\nprint(\"X_train,Y_train:\")\nprint(str(X_train.shape)+\",\"+str(Y_train.shape))\nprint(\"X_test, Y_test\")\nprint(str(X_test.shape)+\", \"+str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling the grayscale value of each pixel.\n#the value of a pixel ranges between 0 and 255\n#in the following code I'll normalize the value to range within 0 and 1\nX_train = X_train / 255\nX_test = X_test / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the output variable is an integer between 0 and 9\n#there are 10 output classes, since there are 10 digits\n#I'll do a one hot encoding of the output classes and transform the vector of class integers into a binary matrix\nprint(Y_train.shape)\nY_train = np_utils.to_categorical(Y_train)\nY_test = np_utils.to_categorical(Y_test)\nprint(Y_train.shape)\nnumOfClasses = Y_train.shape[1]\nprint(numOfClasses)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NN model(one hidden layer with 784 weights and one softmax output layer)\ndef baseline_model(optimizer):\n    model = Sequential()\n    #kernel intializer intializes the weights of a layer\n    model.add(Dense(units=pixelNum, input_dim=pixelNum, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(units=numOfClasses, kernel_initializer='normal', activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = baseline_model('adam')\nmodel.fit(X_train, Y_train, validation_data=(X_test,Y_test), epochs=10, batch_size=200)\n\nscores = model.evaluate(X_test, Y_test)\nprint(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = X_test.reshape(X_test.shape[0], 28,28)\nx_test *= 255\nplt.imshow(x_test[1093], cmap=plt.get_cmap('gray'))\nprint(Y_test[1093])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the following code snippet I'll build a simple ConvNet to recognize digits\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\n\n#this implementation uses Convolutional layer, max pooling layer and Dropout layer\n(X_train,Y_train), (X_test,Y_test) = mnist.load_data('/kaggle/input/mnist-numpy/mnist.npz')\n#2D convolution in keras expects the input/output arrays to be in [m][n_H][n_W][n_C] format\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1).astype('float32')\n# print(X_train.shape)\n# print(X_test.shape)\n\n#normalizing pixel values\nX_train = X_train/255\nX_test = X_test/255\n#one hot encoding for output\nY_train = np_utils.to_categorical(Y_train)\nY_test = np_utils.to_categorical(Y_test)\nnumOfClasses = Y_test.shape[1]\nnumOfPixels = X_train.shape[1]*X_train.shape[2]\n# print(numOfClasses)\n# print(numOfPixels)\n\ndef cnn_model():\n    model = Sequential()\n    #1st layer is Conv layer with n_C = 32, f = 5, n_C_prev = 1, s = 1. p=0\n    model.add(Conv2D(32, (5,5), input_shape=(28,28,1), activation='relu' ))\n    #2nd layer is max pooling layer with n_C = n_C_prev = 32, f = 2, s =1, p =0\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    #3rd layer is a Dropout layer which randomly discards 20% of the nuerons\n    model.add(Dropout(0.2))\n    #4th layer is a Flatten layer which flattens a matrix into a vector which can be process by a Fully Connected Layer\n    model.add(Flatten())\n    #5th layer is a dense layer with 128 neurons\n    model.add(Dense(units=128, activation='relu'))\n    #6th layer is a softmax layer\n    model.add(Dense(units=10, activation='softmax'))\n    \n    #categorical cross entropy denotes logarithmic loss\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n\nmodel = cnn_model()\n#batch size 200 and epoch = 10 translates into 3000 epochs((60000/200)*10)\nmodel.fit(X_train, Y_train, validation_data=(X_test,Y_test), epochs=10, batch_size=200)\nscores = model.evaluate(X_test, Y_test)\n\nprint(100-scores[1]*100)\n","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Train on 60000 samples, validate on 10000 samples\nEpoch 1/10\n60000/60000 [==============================] - 6s 101us/step - loss: 0.2375 - accuracy: 0.9304 - val_loss: 0.0746 - val_accuracy: 0.9773\nEpoch 2/10\n60000/60000 [==============================] - 2s 27us/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 0.0592 - val_accuracy: 0.9815\nEpoch 3/10\n60000/60000 [==============================] - 2s 27us/step - loss: 0.0514 - accuracy: 0.9848 - val_loss: 0.0412 - val_accuracy: 0.9866\nEpoch 4/10\n60000/60000 [==============================] - 2s 26us/step - loss: 0.0399 - accuracy: 0.9878 - val_loss: 0.0413 - val_accuracy: 0.9871\nEpoch 5/10\n60000/60000 [==============================] - 2s 27us/step - loss: 0.0331 - accuracy: 0.9899 - val_loss: 0.0379 - val_accuracy: 0.9877\nEpoch 6/10\n60000/60000 [==============================] - 2s 27us/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 0.0341 - val_accuracy: 0.9884\nEpoch 7/10\n60000/60000 [==============================] - 2s 27us/step - loss: 0.0223 - accuracy: 0.9928 - val_loss: 0.0339 - val_accuracy: 0.9900\nEpoch 8/10\n60000/60000 [==============================] - 2s 26us/step - loss: 0.0184 - accuracy: 0.9938 - val_loss: 0.0310 - val_accuracy: 0.9895\nEpoch 9/10\n60000/60000 [==============================] - 2s 26us/step - loss: 0.0166 - accuracy: 0.9947 - val_loss: 0.0323 - val_accuracy: 0.9899\nEpoch 10/10\n60000/60000 [==============================] - 2s 26us/step - loss: 0.0132 - accuracy: 0.9959 - val_loss: 0.0305 - val_accuracy: 0.9908\n10000/10000 [==============================] - 1s 70us/step\n0.9199976921081543\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\nplt.imshow(X_test[1990], cmap = plt.get_cmap('gray'))\nprint(Y_test[1990])","execution_count":8,"outputs":[{"output_type":"stream","text":"[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Following is the code for a nearly state of the art CNN model for MNIST dataset\n#this would be a larger model\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\n\n#loading data\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data('/kaggle/input/mnist-numpy/mnist.npz')\n\n#reshaping X_test and X_train to incoprate number of channels, which , in this case is 1(because grayscale)\nX_trainFinal = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2],1).astype('float32')\nX_testFinal = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1).astype('float32')\n\n#normalizing the pixel values\nX_trainFinal = X_trainFinal/255\nX_testFinal = X_testFinal/255\n\n#reshaping output vector into one hot matrix\nY_testFinal = np_utils.to_categorical(Y_test)\nY_trainFinal = np_utils.to_categorical(Y_train)\n\nnumOfClasses = Y_trainFinal.shape[1]\n#print(numOfClasses)\n\ndef betterCNNModel():\n    \n    model = Sequential()\n    model.add(Conv2D(30, (5,5), input_shape=(28,28,1), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(15, (3,3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(units=128, activation='relu'))\n    model.add(Dense(units=50, activation='relu'))\n    model.add(Dense(units=10, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model\n\nmodel = betterCNNModel()\nmodel.fit(X_trainFinal, Y_trainFinal, validation_data=(X_testFinal, Y_testFinal), epochs=10, batch_size=200)\nscores = model.evaluate(X_testFinal, Y_testFinal)\n\nprint(100-scores[1]*100)","execution_count":16,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Error when checking input: expected conv2d_6_input to have 4 dimensions, but got array with shape (60000, 28, 28)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-779d2d0d36b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetterCNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_6_input to have 4 dimensions, but got array with shape (60000, 28, 28)"]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}